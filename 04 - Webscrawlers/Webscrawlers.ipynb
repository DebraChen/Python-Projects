{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b168f-cd83-4406-868f-a7a1876e8503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Google Scholar carwler: can't deal with PDF, others goes normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0614a8-2e14-41f7-817c-d98a3ee61978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install beautifulsoup4\n",
    "#pip install requests \n",
    "\n",
    "#Find webdriver path\n",
    "import os\n",
    "import sys\n",
    "os.path.dirname(sys.executable)\n",
    "\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=metaverse&btnG=\")\n",
    "\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "class DoubanParser:\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    book_dict = {}\n",
    "\n",
    "    def parse(self, page_url):\n",
    "        self.driver.get(page_url)\n",
    "        html = self.driver.page_source\n",
    "        page_soup = BeautifulSoup(html, features='lxml') \n",
    "        book_titles = page_soup.find_all('div', {'class': 'gs_ri'})#   #h3.a is the thing that changes in each web\n",
    "        for title in book_titles:\n",
    "            book_link = title.h3.a#   #h3.a is the thing that changes in each web\n",
    "            title = book_link.get_text().replace(\"\\n\", \"\")\n",
    "            link = book_link[\"href\"]#  #href is the thing that might changes in each web\n",
    "            print(f'{title}:{link}')\n",
    "            book_description = self.get_book_description(link)\n",
    "            self.book_dict[title] = book_description \n",
    "            \n",
    "    def get_book_description(self, page_url):\n",
    "        self.driver.get(page_url)\n",
    "        html = self.driver.page_source\n",
    "        page_soup = BeautifulSoup(html, features='lxml') \n",
    "        book_description = page_soup.find('div', {'class': 'gs_rs'})\n",
    "        text = book_description.p.get_text()\n",
    "        return text\n",
    "\n",
    "    def write_to_csv(self, file_name):\n",
    "        with open(file_name, 'w', newline='') as file:\n",
    "            write = csv.writer(file)\n",
    "            write.writerow(['book_title', 'book_link', 'description'])\n",
    "            for title, link, description in self.book_dict.items():\n",
    "                write.writerow([title, description])\n",
    "\n",
    "doubanParser = DoubanParser()\n",
    "new_page = 'https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=metaverse&btnG='\n",
    "doubanParser.parse(new_page)\n",
    "\n",
    "doubanParser.write_to_csv('meta-books.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853fce9d-cc66-4228-b824-d409107c2379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Douban Chinese Web scarwler: goes well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734fbab9-f041-4482-a875-6e9889111ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "class DoubanParser:\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    book_dict = {}\n",
    "\n",
    "    def parse(self, page_url):\n",
    "        self.driver.get(page_url)\n",
    "        html = self.driver.page_source\n",
    "        page_soup = BeautifulSoup(html, features='lxml') \n",
    "        book_titles = page_soup.find_all('div', {'class': 'info'})\n",
    "        for title in book_titles:\n",
    "            book_link = title.h2.a\n",
    "            title = book_link.get_text().replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "            link = book_link[\"href\"]\n",
    "            print(f'{title}:{link}')\n",
    "            book_description = self.get_book_description(link)\n",
    "            self.book_dict[title] = book_description \n",
    "\n",
    "    def get_book_description(self, page_url):\n",
    "        self.driver.get(page_url)\n",
    "        html = self.driver.page_source\n",
    "        page_soup = BeautifulSoup(html, features='lxml') \n",
    "        book_description = page_soup.find('div', {'class': 'intro'})\n",
    "        text = book_description.p.get_text()\n",
    "        return text\n",
    "\n",
    "    def write_to_csv(self, file_name):\n",
    "        with open(file_name, 'w', newline='') as file:\n",
    "            write = csv.writer(file)\n",
    "            write.writerow(['book_title', 'description'])\n",
    "            for title, description in self.book_dict.items():\n",
    "                write.writerow([title, description])\n",
    "\n",
    "doubanParser = DoubanParser()\n",
    "new_page = 'https://book.douban.com/tag/%E7%BC%96%E7%A8%8B'\n",
    "doubanParser.parse(new_page)\n",
    "\n",
    "doubanParser.write_to_csv('books.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3609805-ca8a-47a6-ba57-fb6cee0528e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHALLENGEï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073b2da-0fe0-4179-8d60-b61fb8a879dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sky news: site is not defined, use - makes code goes wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8983db-3a66-42fc-aa78-b5e664c357e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install beautifulsoup4\n",
    "#pip install requests \n",
    "\n",
    "#Find webdriver path\n",
    "import os\n",
    "import sys\n",
    "os.path.dirname(sys.executable)\n",
    "\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://news.sky.com/world\")\n",
    "\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "class DoubanParser:\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    book_dict = {}\n",
    "\n",
    "# Home page, grab news titles and links\n",
    "    def parse(self, page_url):\n",
    "        self.driver.get(page_url)\n",
    "        html = self.driver.page_source\n",
    "        page_soup = BeautifulSoup(html, features='lxml') \n",
    "        book_titles = page_soup.find_all('div', {'class': 'sdc-site-tile__body-main'})#   #h3.a is the thing that changes in each web\n",
    "        for title in book_titles:\n",
    "            book_link = title.span.sdc-site-tile__headline-text#   #h3.a is the thing that changes in each web\n",
    "            title = book_link.get_text().replace(\"\\n\", \"\")\n",
    "            link = book_link[\"href\"]#  #href is the thing that might changes in each web\n",
    "            print(f'{title}:{link}')\n",
    "            book_description = self.get_book_description(link)\n",
    "            self.book_dict[title] = book_description \n",
    "    \n",
    "# New page of specific news, get the detailed description\n",
    "    def get_book_description(self, page_url):\n",
    "        self.driver.get(page_url)\n",
    "        html = self.driver.page_source\n",
    "        page_soup = BeautifulSoup(html, features='lxml') \n",
    "        book_description = page_soup.find('div', {'class': 'sdc-site-component-header--h2 sdc-article-header__sub-title'})\n",
    "        text = book_description.p.get_text()\n",
    "        return text\n",
    "\n",
    "# Write those info into csv, search on computer with '.csv' to find it.\n",
    "# Hint: remember to change the file name for each time\n",
    "    def write_to_csv(self, file_name):\n",
    "        with open(file_name, 'w', newline='') as file:\n",
    "            write = csv.writer(file)\n",
    "            write.writerow(['book_title', 'book_link', 'description'])\n",
    "            for title, link, description in self.book_dict.items():\n",
    "                write.writerow([title, description])\n",
    "\n",
    "doubanParser = DoubanParser()\n",
    "new_page = 'https://news.sky.com/world'\n",
    "doubanParser.parse(new_page)\n",
    "\n",
    "doubanParser.write_to_csv('news.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f108d-0ecd-4f11-8dfc-1e18d5122518",
   "metadata": {},
   "outputs": [],
   "source": [
    "Define names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05caf064-1f00-4e2a-b5e9-3c6468932c87",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in ./Documents/anaconda3/lib/python3.9/site-packages (4.1.2)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in ./Documents/anaconda3/lib/python3.9/site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in ./Documents/anaconda3/lib/python3.9/site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in ./Documents/anaconda3/lib/python3.9/site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: sortedcontainers in ./Documents/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in ./Documents/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in ./Documents/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in ./Documents/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: sniffio in ./Documents/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in ./Documents/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in ./Documents/anaconda3/lib/python3.9/site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in ./Documents/anaconda3/lib/python3.9/site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in ./Documents/anaconda3/lib/python3.9/site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in ./Documents/anaconda3/lib/python3.9/site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in ./Documents/anaconda3/lib/python3.9/site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: cffi>=1.12 in ./Documents/anaconda3/lib/python3.9/site-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.14.6)\n",
      "Requirement already satisfied: pycparser in ./Documents/anaconda3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.20)\n",
      "Requirement already satisfied: six>=1.5.2 in ./Documents/anaconda3/lib/python3.9/site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in ./Documents/anaconda3/lib/python3.9/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./Documents/anaconda3/lib/python3.9/site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./Documents/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.2.1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lf/36j9h0c17ps5kff9prjl7wz40000gn/T/ipykernel_10633/545690313.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mdoubanParser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoubanParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mnew_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://news.sky.com/world'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mdoubanParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mdoubanParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'news.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lf/36j9h0c17ps5kff9prjl7wz40000gn/T/ipykernel_10633/545690313.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, page_url)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mbook_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSPAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m#   #h3.a is the thing that changes in each web\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbook_link\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mlink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbook_link\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"href\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#  #href is the thing that might changes in each web\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{title}:{link}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install beautifulsoup4\n",
    "#pip install requests \n",
    "\n",
    "#Find webdriver path\n",
    "import os\n",
    "import sys\n",
    "os.path.dirname(sys.executable)\n",
    "\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://news.sky.com/world\")\n",
    "\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "SDC1 = 'sdc-site-tile__body-main'\n",
    "SDC2 = 'sdc-site-component-header--h2 sdc-article-header__sub-title'\n",
    "SPAN = 'span.sdc-site-tile__headline-text'\n",
    "class DoubanParser:\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    book_dict = {}\n",
    "\n",
    "# Home page, grab news titles and links\n",
    "    def parse(self, page_url):\n",
    "        self.driver.get(page_url)\n",
    "        html = self.driver.page_source\n",
    "        page_soup = BeautifulSoup(html, features='lxml') \n",
    "        book_titles = page_soup.find_all('div', {'class': (SDC1)})#   #h3.a is the thing that changes in each web\n",
    "        for title in book_titles:\n",
    "            book_link = title.SPAN\n",
    "            #   #h3.a is the thing that changes in each web\n",
    "            title = book_link.get_text().replace(\"\\n\", \"\")\n",
    "            link = book_link[\"href\"]#  #href is the thing that might changes in each web\n",
    "            print(f'{title}:{link}')\n",
    "            book_description = self.get_book_description(link)\n",
    "            self.book_dict[title] = book_description \n",
    "    \n",
    "# New page of specific news, get the detailed description\n",
    "    def get_book_description(self, page_url):\n",
    "        self.driver.get(page_url)\n",
    "        html = self.driver.page_source\n",
    "        page_soup = BeautifulSoup(html, features='lxml') \n",
    "        book_description = page_soup.find('div', {'class': (SDC2)})\n",
    "        text = book_description.p.get_text()\n",
    "        return text\n",
    "\n",
    "# Write those info into csv, search on computer with '.csv' to find it.\n",
    "# Hint: remember to change the file name for each time\n",
    "    def write_to_csv(self, file_name):\n",
    "        with open(file_name, 'w', newline='') as file:\n",
    "            write = csv.writer(file)\n",
    "            write.writerow(['book_title', 'book_link', 'description'])\n",
    "            for title, link, description in self.book_dict.items():\n",
    "                write.writerow([title, description])\n",
    "\n",
    "doubanParser = DoubanParser()\n",
    "new_page = 'https://news.sky.com/world'\n",
    "doubanParser.parse(new_page)\n",
    "\n",
    "doubanParser.write_to_csv('news.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ac92d-ab98-45ed-8602-9b9770a93913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e5dfb2-abe5-4a42-b15c-22dd487b402f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e827752-de0a-4aaf-bcb5-bc03d303dad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b680f020-c500-4217-ae61-70716c6c528d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
